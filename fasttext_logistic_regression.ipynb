{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["1. Get a training corpus\n","2. Tokenize the corpus and set a vocabulary for an embedding model\n","3. Train the embedding model\n","4. Set a classification model\n","5. Use the trained embedding model for the classification task"],"metadata":{"id":"wQFqLaKkq-nK"}},{"cell_type":"code","source":["# numpy를 호환 가능한 버전으로 강제로 재설치\n","!pip install numpy==1.23.5 --force-reinstall\n","\n","# gensim도 재설치\n","!pip install gensim --force-reinstall"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":684},"id":"VaPA0cP_d4dz","executionInfo":{"status":"ok","timestamp":1744658816065,"user_tz":-540,"elapsed":13759,"user":{"displayName":"­김시현 / 학생 / 경영학과","userId":"10111927817961129294"}},"outputId":"b47f0f15-cf2a-4ad0-d300-8b1f8c93e979"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting numpy==1.23.5\n","  Using cached numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n","Using cached numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n","Installing collected packages: numpy\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.26.4\n","    Uninstalling numpy-1.26.4:\n","      Successfully uninstalled numpy-1.26.4\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","blosc2 3.2.1 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n","jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n","chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n","pymc 5.21.2 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n","bigframes 1.42.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n","jax 0.5.2 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n","scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n","treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n","thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.23.5 which is incompatible.\n","tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.23.5 which is incompatible.\n","imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n","albumentations 2.0.5 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n","albucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n","xarray 2025.1.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed numpy-1.23.5\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]},"id":"adbd9790434d43e18672b31218f06abd"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting gensim\n","  Using cached gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n","    status = run_func(*args)\n","^C\n"]}]},{"cell_type":"code","source":["from gensim.models import FastText\n","from gensim.utils import simple_preprocess\n","\n","import numpy as np\n","from collections import Counter\n","\n","import torch\n","import torch.nn as nn\n","from torch.optim import Adam\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","from sklearn.metrics import f1_score"],"metadata":{"id":"x1OXYR4ltPH0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1. Get a training corpus\n","\n","movie review corpus (MR).\n","\n"],"metadata":{"id":"1Xt7EafBrW9y"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yaE7JEOvpxoI","outputId":"d7792dec-1bbd-41a5-9a1c-57909fcf60ba","executionInfo":{"status":"ok","timestamp":1744658839009,"user_tz":-540,"elapsed":1343,"user":{"displayName":"­김시현 / 학생 / 경영학과","userId":"10111927817961129294"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-04-14 19:27:18--  https://github.com/FKarl/short-text-classification/raw/refs/heads/main/data/corpus/mr_shuffle.txt\n","Resolving github.com (github.com)... 20.27.177.113\n","Connecting to github.com (github.com)|20.27.177.113|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://raw.githubusercontent.com/FKarl/short-text-classification/refs/heads/main/data/corpus/mr_shuffle.txt [following]\n","--2025-04-14 19:27:18--  https://raw.githubusercontent.com/FKarl/short-text-classification/refs/heads/main/data/corpus/mr_shuffle.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1200403 (1.1M) [text/plain]\n","Saving to: ‘mr_shuffle.txt.1’\n","\n","mr_shuffle.txt.1    100%[===================>]   1.14M  4.20MB/s    in 0.3s    \n","\n","2025-04-14 19:27:19 (4.20 MB/s) - ‘mr_shuffle.txt.1’ saved [1200403/1200403]\n","\n"]}],"source":["# download the file\n","!wget https://github.com/FKarl/short-text-classification/raw/refs/heads/main/data/corpus/mr_shuffle.txt"]},{"cell_type":"code","source":["# we have the file in current location: mr_shuffle.txt\n","!ls -alh"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7dj4Qk91qmgc","outputId":"14fe1c18-0f6f-4261-80d0-13a908238047","executionInfo":{"status":"ok","timestamp":1744658841595,"user_tz":-540,"elapsed":108,"user":{"displayName":"­김시현 / 학생 / 경영학과","userId":"10111927817961129294"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["total 780M\n","drwxr-xr-x 1 root root 4.0K Apr 14 19:27 .\n","drwxr-xr-x 1 root root 4.0K Apr 14 10:44 ..\n","drwxr-xr-x 4 root root 4.0K Apr 10 13:37 .config\n","-rw-r--r-- 1 root root 7.9M Apr 14 19:16 core_emb\n","-rw-r--r-- 1 root root 763M Apr 14 19:16 core_emb.wv.vectors_ngrams.npy\n","drwxr-xr-x 2 root root 4.0K Apr 14 18:03 .ipynb_checkpoints\n","-rw-r--r-- 1 root root 1.2M Apr 14 17:48 mr_shuffle.txt\n","-rw-r--r-- 1 root root 1.2M Apr 14 19:27 mr_shuffle.txt.1\n","drwxr-xr-x 1 root root 4.0K Apr 10 13:37 sample_data\n","-rw-r--r-- 1 root root 5.4M Apr 14 18:40 sst2_model.pt\n","-rw-r--r-- 1 root root 185K Apr 14 17:02 sst2-test.txt\n","-rw-r--r-- 1 root root 704K Apr 14 16:58 sst2-train.txt\n"]}]},{"cell_type":"code","source":["# check the first 10 lines of the mr_shuffle.txt file\n","!head mr_shuffle.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yhmrueSuqqVV","outputId":"4d4853c6-378e-49d2-fa4a-7c75209a6abf","executionInfo":{"status":"ok","timestamp":1744658844017,"user_tz":-540,"elapsed":138,"user":{"displayName":"­김시현 / 학생 / 경영학과","userId":"10111927817961129294"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["uncertain in tone a garbled exercise in sexual politics , a junior varsity short cuts by way of very bad things\n","weighted down with slow , uninvolving storytelling and flat acting\n","the tale of her passionate , tumultuous affair with musset unfolds as sand 's masculine persona , with its love of life and beauty , takes form\n","anyone who suffers through this film deserves , at the very least , a big box of consolation candy\n","it 's a gag that 's worn a bit thin over the years , though do n't ask still finds a few chuckles\n","paid in full is remarkably engaging despite being noticeably derivative of goodfellas and at least a half dozen other trouble in the ghetto flicks\n","you have no affinity for most of the characters nothing about them is attractive what they see in each other also is difficult to fathom\n","the best thing i can say about this film is that i ca n't wait to see what the director does next\n","when you think you 've figured out bielinsky 's great game , that 's when you 're in the most trouble he 's the con , and you 're just the mark\n","these are textbook lives of quiet desperation\n"]}]},{"cell_type":"code","source":["# check the size of the test.tsv file: it has 10,661 lines\n","!wc mr_shuffle.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RjjH3YAksEl3","outputId":"a9ee1c07-5948-473d-d127-18e76bccb52f","executionInfo":{"status":"ok","timestamp":1744658846008,"user_tz":-540,"elapsed":136,"user":{"displayName":"­김시현 / 학생 / 경영학과","userId":"10111927817961129294"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  10661  217354 1200403 mr_shuffle.txt\n"]}]},{"cell_type":"markdown","source":["## 2. Tokenize the corpus and set a vocabulary for an embedding model"],"metadata":{"id":"nPxj_OOtu0in"}},{"cell_type":"code","source":["# text processing\n","sentences = []\n","with open('mr_shuffle.txt', 'r') as f:\n","    for line in f:\n","        line = line.strip()\n","        if line:\n","            tokens = simple_preprocess(line)\n","            sentences.append(tokens)\n","\n"," # FastText는 단어를 내부적으로 subword 단위로 쪼개서 학습하므로 외부에서 굳이 단어보다 더 작게 자를 필요가 없어서\n"," # 간단한 단어 수준의 tokenization 방식인 simple_preprocess을 채택했습니다."],"metadata":{"id":"QEm0dhF8qve-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##3. train the embedding model"],"metadata":{"id":"z9KVS62XzWKe"}},{"cell_type":"code","source":["# FastText 선택 이유: 영화 리뷰에는 구어체에서 쓰이는 희귀한 단어, 오타, 비표준어 등이 많을 것으로 추측되므로,\n","# 단어를 subword로 쪼개서 학습하여 OOV에도 어느 정도 의미있는 벡터를 제공하는 FastText가 적합할 것이라고 판단했습니다.\n","\n","embedding_model = FastText(\n","    sentences=sentences,\n","    vector_size=100, # 각 단어 벡터 차원 수\n","    window=5, # 앞뒤로 5단어까지 문맥으로 간주\n","    min_count=2, # 최소 등장 횟수가 2번 이상인 단어만 학습\n","    workers=4, # 학습에 사용할 쓰레드 수\n","    sg=1, # Skip-gram 모델 사용 (희귀한 단어를 더 잘 다루고자 선택했습니다)\n","    epochs=10 # 학습 반복 횟수\n",")"],"metadata":{"id":"nROmoWgguu84"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# save the trained embedding model\n","embedding_model.save(\"core_emb\")"],"metadata":{"id":"jVln-R5Wu_-Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4. Set a classification model"],"metadata":{"id":"tE1z5Ut4ux_6"}},{"cell_type":"markdown","source":["classification pipeline using a simple Logistic Regression model.\n","\n","* Task: SST2 (Stanford Sentiment Treebank; Binary Classification)\n","* Text: movie review for sentiment classification\n","* Label: 1 (Positive), 0 (Negative)"],"metadata":{"id":"Uu1VEDpcvIbi"}},{"cell_type":"code","source":["# training set\n","\n","!wget https://github.com/FKarl/short-text-classification/raw/refs/heads/main/data/sst2/sst2-train.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mze_djrAveOk","outputId":"6b3a8fa9-485d-490b-d374-b9bc941930c8","executionInfo":{"status":"ok","timestamp":1744658893864,"user_tz":-540,"elapsed":1802,"user":{"displayName":"­김시현 / 학생 / 경영학과","userId":"10111927817961129294"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-04-14 19:28:12--  https://github.com/FKarl/short-text-classification/raw/refs/heads/main/data/sst2/sst2-train.txt\n","Resolving github.com (github.com)... 20.27.177.113\n","Connecting to github.com (github.com)|20.27.177.113|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://raw.githubusercontent.com/FKarl/short-text-classification/refs/heads/main/data/sst2/sst2-train.txt [following]\n","--2025-04-14 19:28:13--  https://raw.githubusercontent.com/FKarl/short-text-classification/refs/heads/main/data/sst2/sst2-train.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 720259 (703K) [text/plain]\n","Saving to: ‘sst2-train.txt.1’\n","\n","sst2-train.txt.1    100%[===================>] 703.38K  2.77MB/s    in 0.2s    \n","\n","2025-04-14 19:28:14 (2.77 MB/s) - ‘sst2-train.txt.1’ saved [720259/720259]\n","\n"]}]},{"cell_type":"code","source":["# test set\n","\n","!wget https://github.com/FKarl/short-text-classification/raw/refs/heads/main/data/sst2/sst2-test.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O_YESCgw_b9J","outputId":"e22fee3d-4cfb-4138-90b4-8711968b2bb3","executionInfo":{"status":"ok","timestamp":1744658898257,"user_tz":-540,"elapsed":1112,"user":{"displayName":"­김시현 / 학생 / 경영학과","userId":"10111927817961129294"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-04-14 19:28:17--  https://github.com/FKarl/short-text-classification/raw/refs/heads/main/data/sst2/sst2-test.txt\n","Resolving github.com (github.com)... 20.27.177.113\n","Connecting to github.com (github.com)|20.27.177.113|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://raw.githubusercontent.com/FKarl/short-text-classification/refs/heads/main/data/sst2/sst2-test.txt [following]\n","--2025-04-14 19:28:18--  https://raw.githubusercontent.com/FKarl/short-text-classification/refs/heads/main/data/sst2/sst2-test.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 188749 (184K) [text/plain]\n","Saving to: ‘sst2-test.txt.1’\n","\n","sst2-test.txt.1     100%[===================>] 184.33K  1.16MB/s    in 0.2s    \n","\n","2025-04-14 19:28:18 (1.16 MB/s) - ‘sst2-test.txt.1’ saved [188749/188749]\n","\n"]}]},{"cell_type":"markdown","source":["two columns in each dataset file:\n","* label: 1 (positive), 0 (negative)\n","* movie review text"],"metadata":{"id":"5z2ut66W_m3r"}},{"cell_type":"code","source":["# check the sample lines and columns\n","\n","!head sst2-train.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UQytn0Lz_haG","outputId":"312612bf-0911-4a43-81c1-127e71744784","executionInfo":{"status":"ok","timestamp":1744658901699,"user_tz":-540,"elapsed":104,"user":{"displayName":"­김시현 / 학생 / 경영학과","userId":"10111927817961129294"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1\ta stirring , funny and finally transporting re imagining of beauty and the beast and 1930s horror films\n","0\tapparently reassembled from the cutting room floor of any given daytime soap\n","0\tthey presume their audience wo n't sit still for a sociology lesson , however entertainingly presented , so they trot out the conventional science fiction elements of bug eyed monsters and futuristic women in skimpy clothes\n","1\tthis is a visually stunning rumination on love , memory , history and the war between art and commerce\n","1\tjonathan parker 's bartleby should have been the be all end all of the modern office anomie films\n","1\tcampanella gets the tone just right funny in the middle of sad in the middle of hopeful\n","0\ta fan film that for the uninitiated plays better on video with the sound turned down\n","1\tb art and berling are both superb , while huppert is magnificent\n","0\ta little less extreme than in the past , with longer exposition sequences between them , and with fewer gags to break the tedium\n","0\tthe film is strictly routine\n"]}]},{"cell_type":"code","source":["# check the data sizes\n","\n","!wc sst2-train.txt # 6919 lines\n","!wc sst2-test.txt # 1820 lines"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TnfiynAv_66u","outputId":"d7ef47e8-7dad-4897-93b4-36535fa7525e","executionInfo":{"status":"ok","timestamp":1744658904763,"user_tz":-540,"elapsed":222,"user":{"displayName":"­김시현 / 학생 / 경영학과","userId":"10111927817961129294"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  6919 135016 720259 sst2-train.txt\n","  1820  35474 188749 sst2-test.txt\n"]}]},{"cell_type":"code","source":["# convert texts to sequences\n","\n","sentences = []\n","y_train = []\n","\n","# training data 불러오기\n","with open('sst2-train.txt', 'r') as f:\n","    for line in f:\n","        line = line.strip()\n","        if not line:\n","            continue\n","        label, text = line.split('\\t', 1)\n","        y_train.append(int(label))\n","        sentences.append(text)\n","\n","# tokenization\n","tokenized_sentences = [sent.split() for sent in sentences]\n","\n","# 등장 빈도순으로 정렬\n","word_list = []\n","for sent in tokenized_sentences:\n","    for word in sent:\n","      word_list.append(word)\n","\n","word_counts = Counter(word_list)\n","\n","# vocab 생성\n","vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n","\n","word_to_index = {}\n","word_to_index['<PAD>'] = 0\n","word_to_index['<UNK>'] = 1\n","\n","for index, word in enumerate(vocab) :\n","  word_to_index[word] = index + 2\n","\n","vocab_size = len(word_to_index)\n","\n","# 텍스트를 시퀀스로 변환\n","def texts_to_sequences(tokenized_X_data, word_to_index):\n","  encoded_X_data = []\n","  for sent in tokenized_X_data:\n","    index_sequences = []\n","    for word in sent:\n","      try:\n","          index_sequences.append(word_to_index[word])\n","      except KeyError:\n","          index_sequences.append(word_to_index['<UNK>']) # vocabulary에 없는 단어라면 unknown에 해당하는 수로 바꾸기.\n","    encoded_X_data.append(index_sequences)\n","  return encoded_X_data\n","\n","X_encoded = texts_to_sequences(tokenized_sentences, word_to_index)\n","max_len = max(len(l) for l in X_encoded)\n","\n","# 시퀀스 padding\n","def pad_sequences(sentences, max_len):\n","  features = np.zeros((len(sentences), max_len), dtype=int)\n","  for index, sentence in enumerate(sentences):\n","    if len(sentence) != 0:\n","      features[index, :len(sentence)] = np.array(sentence)[:max_len]\n","  return features\n","\n","X_train = pad_sequences(X_encoded, max_len=max_len)\n","y_train = np.array(y_train)"],"metadata":{"id":"OLfkGJWBACTU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define the model class\n","# define the 'Logistic Regression' model you learned!\n","\n","# 앞서 학습시킨 FastText 임베딩 로드\n","ft_model = FastText.load(\"core_emb\")\n","\n","# 2차원 임베딩 매트릭스 생성\n","embedding_matrix = np.zeros((len(word_to_index), ft_model.vector_size))\n","\n","for word, idx in word_to_index.items():\n","    if word in ft_model.wv:\n","        embedding_matrix[idx] = ft_model.wv[word]\n","    else:\n","        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(ft_model.vector_size,))  # OOV\n","\n","\n","class SimpleModel(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim):\n","        super(SimpleModel, self).__init__()\n","        # embedding layer에 앞서 만든 임베딩 적용.\n","        # freeze=False : 임베딩 파인튜닝 허용. 사전 학습된 임베딩을 학습 중에도 업데이트할 수 있게 함.\n","        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32), freeze=False)\n","         # linear layer는 1차원을 받으므로 2차원 embedding을 1차원으로 펼침.\n","        self.flatten = nn.Flatten()\n","        # linear layer 1개. 문장이 들어왔을 때 positive한 layer로 나올 확률.\n","        self.fc = nn.Linear(embedding_dim * max_len, 1)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        # embedded.shape == (배치 크기, 문장의 길이, 임베딩 벡터의 차원)\n","        embedded = self.embedding(x)\n","\n","        # flattend.shape == (배치 크기, 문장의 길이 × 임베딩 벡터의 차원)\n","        flattened = self.flatten(embedded)\n","\n","        # output.shape == (배치 크기, 1)\n","        output = self.fc(flattened)\n","        return self.sigmoid(output)"],"metadata":{"id":"JLEIfsmQAW6l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# instantiate the model, criterion(loss), and optimizer\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","embedding_dim = 100\n","simple_model = SimpleModel(vocab_size, embedding_dim).to(device)\n","\n","# nn에서 loss를 골라와서 세팅해준다.\n","# BCELoss는 이진 분류 문제에 적합한 loss function이고 sigmoid와 함께 사용되어 선택했습니다.\n","criterion = nn.BCELoss()\n","\n","# parameter를 optimizer에 넣어준다.\n","# Adam은 별다른 튜닝 없이도 좋은 성능을 내는 대표적인 최적화 알고리즘이라 선택했습니다.\n","optimizer = Adam(simple_model.parameters())\n","\n","train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.long), torch.tensor(y_train, dtype=torch.float32))\n","train_dataloader = DataLoader(train_dataset, batch_size=2) # batch 단위로 묶어준다. 여기에서는 데이터 2개를 한 묶음으로 만들어 처리한다."],"metadata":{"id":"gREX5BqlAYen","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##5. use the trained embedding model for the classification task"],"metadata":{"id":"C0NTU37b0E_x"}},{"cell_type":"code","source":["# train the model!\n","# train the model for 5 epochs\n","# and print the training loss at the end of each epoch\n","\n","for epoch in range(5):\n","    # input: index로 되어있는 문장. for문이 2번이므로 training data 내 모든 batch에 대해 반복한다.\n","    for inputs, targets in train_dataloader:\n","        # inputs.shape == (배치 크기, 문장 길이)\n","        # targets.shape == (배치 크기)\n","        inputs, targets = inputs.to(device), targets.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        # outputs.shape == (배치 크기)\n","        outputs = simple_model(inputs).view(-1) # 이 자체가 확률이 된다.\n","\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","\n","        optimizer.step()\n","\n","    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"],"metadata":{"id":"s1LkfGtiAclE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744659406207,"user_tz":-540,"elapsed":475162,"user":{"displayName":"­김시현 / 학생 / 경영학과","userId":"10111927817961129294"}},"outputId":"824a2d62-818d-4b5e-ce7c-05c3d6b20d74"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1, Loss: 0.2230575829744339\n","Epoch 2, Loss: 0.14312806725502014\n","Epoch 3, Loss: 0.06967110931873322\n","Epoch 4, Loss: 0.006739622447639704\n","Epoch 5, Loss: 0.0008573121158406138\n"]}]},{"cell_type":"code","source":["# save the trained model\n","# filename: 'sst2_model.pt'\n","torch.save(simple_model.state_dict(), 'sst2_model.pt')"],"metadata":{"id":"qGgY0Zm0BeU4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* Use `f1 score` for the evaluation metric.\n","\n","  * See also: https://scikit-learn.org/0.15/modules/generated/sklearn.metrics.f1_score.html\n","\n","  * Use 'macro average'!\n","  * The output will be in the form of a single scalar (numerical) value."],"metadata":{"id":"iZWdJZs7A03Q"}},{"cell_type":"code","source":["# convert test texts to sequences\n","\n","x_test = []\n","y_test = []\n","\n","with open('sst2-test.txt', 'r') as f:\n","    for line in f:\n","        line = line.strip()\n","        if not line:\n","            continue\n","        label, text = line.split('\\t', 1)\n","        y_test.append(int(label))\n","        x_test.append(text)\n","\n","x_test = [i.split() for i in x_test] # test data를 tokenize한다.\n","x_test = texts_to_sequences(x_test, word_to_index) # index로 바꿔준다. vocabulary에 없는 것은 unknown으로 처리된다.\n","x_test = pad_sequences(x_test, max_len=max_len)\n","y_test = np.array(y_test)\n","\n","test_dataset = TensorDataset(torch.tensor(x_test, dtype=torch.long), torch.tensor(y_test, dtype=torch.float32))\n","test_dataloader = DataLoader(test_dataset, batch_size=2)"],"metadata":{"id":"LMOWARsgBOE0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# test the model\n","# print the f1 score for the test dataset\n","y_true = []\n","y_pred = []\n","\n","# 모델을 evaluation mode로 바꿔준다. 성능을 테스트할 것이므로 더이상 gradient update을 하지 않겠다는 뜻이다.\n","simple_model.eval()\n","\n","for inputs, targets in test_dataloader:\n","    # inputs.shape == (배치 크기, 문장 길이)\n","    # targets.shape == (배치 크기)\n","    inputs, targets = inputs.to(device), targets.to(device)\n","\n","    # outputs.shape == (배치 크기)\n","    outputs = simple_model(inputs).view(-1)\n","    preds = (outputs > 0.5).long().view(-1)\n","\n","    y_pred.extend(preds.tolist())\n","    y_true.extend(targets.tolist())\n","\n","\n","# test dataset에 대한 f1 score\n","print(f1_score(y_true, y_pred, average='macro'))"],"metadata":{"id":"tb9SJIwQBYbg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744659757526,"user_tz":-540,"elapsed":682,"user":{"displayName":"­김시현 / 학생 / 경영학과","userId":"10111927817961129294"}},"outputId":"b53477c6-e02a-4034-b15d-757047547d3e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.758868892727763\n"]}]}]}